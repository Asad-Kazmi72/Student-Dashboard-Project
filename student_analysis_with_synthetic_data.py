# -*- coding: utf-8 -*-
"""Student Analysis With Synthetic Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZpdwVdQRadr4TipIFhCPegVv3kChSQR
"""

import pandas as pd
import numpy as np

# Set seed
np.random.seed(42)

# Fixed list of 25 students
num_students = 25
roll_numbers = [f"Student_{i+1:03d}" for i in range(num_students)]

# Function to generate synthetic student data
def generate_student_data(roll_numbers, course_name, semester, quiz_count, pa_count,
                           project_max, mid_max, final_max, quiz_weight, pa_weight,
                           project_weight, mid_weight, final_weight):
    data = []
    for roll in roll_numbers:
        quizzes = np.random.uniform(0, 10, quiz_count)
        pas = np.random.uniform(0, 100, pa_count)
        project = np.random.uniform(0, project_max)
        mid = np.random.uniform(0, mid_max)
        final = np.random.uniform(0, final_max)

        quiz_total = quizzes.sum()
        pa_total = pas.sum()

        weighted_score = (
            (quiz_total / (quiz_count * 10)) * quiz_weight +
            (pa_total / (pa_count * 100)) * pa_weight +
            (project / project_max) * project_weight +
            (mid / mid_max) * mid_weight +
            (final / final_max) * final_weight
        ) * 100

        data.append([roll, quiz_total, pa_total, project, mid, final, round(weighted_score, 2), course_name, semester])

    return pd.DataFrame(data, columns=[
        "Roll Number", "Quiz Total", "PA Total", "Project",
        "Mid Exam", "Final Exam", "Weighted Score", "Course", "Semester"
    ])

# Course parameters
courses = [
    ("Advanced ML", "Sem 1", 4, 3, 100, 40, 45, 0.2, 0.1, 0.1, 0.25, 0.35),
    ("Deep Learning", "Sem 1", 4, 3, 100, 50, 60, 0.2, 0.1, 0.1, 0.25, 0.35),
    ("Data Mining", "Sem 1", 2, 2, 15, 55, 60, 0.15, 0.1, 0.15, 0.25, 0.35),
    ("Stats and Maths", "Sem 2", 4, 4, 10, 50, 50, 0.1, 0.1, 0.1, 0.3, 0.4)
]

# Generate and merge
all_dfs = [generate_student_data(roll_numbers, *course) for course in courses]
merged_df = pd.concat(all_dfs, ignore_index=True)

# Output shape and preview
merged_df.shape, merged_df

# Check the structure
merged_df.info()

# Preview the data
merged_df.head()

# Check for missing values
merged_df.isnull().sum()

# Unique students and courses
print("Unique students:", merged_df['Roll Number'].nunique())
print("Courses:", merged_df['Course'].unique())

student_course_counts = merged_df.groupby("Roll Number")["Course"].count().sort_values(ascending=False)
print(student_course_counts)

import seaborn as sns
import matplotlib.pyplot as plt

# Histogram of Weighted Scores
plt.figure(figsize=(10,5))
sns.histplot(data=merged_df, x="Weighted Score", bins=15, kde=True, hue="Course", multiple="stack")
plt.title("Distribution of Weighted Scores by Course")
plt.show()

course_summary = merged_df.groupby("Course").describe().T
print(course_summary)

# Boxplot to compare scores across courses
plt.figure(figsize=(10,6))
sns.boxplot(x="Course", y="Weighted Score", data=merged_df)
plt.title("Weighted Score Distribution Across Courses")
plt.show()

pivot_df = merged_df.pivot(index="Roll Number", columns="Course", values="Weighted Score")
print(pivot_df)

sns.pairplot(merged_df, hue="Course", vars=["Quiz Total", "PA Total", "Project", "Mid Exam", "Final Exam", "Weighted Score"])
plt.suptitle("Pairwise Relationships Between Assessment Components", y=1.02)
plt.show()

# Top students per course
top_students = merged_df.groupby("Course").apply(lambda x: x.nlargest(3, "Weighted Score"))

top_students

# Average score per student across all courses
average_scores = merged_df.groupby("Roll Number")["Weighted Score"].mean().sort_values(ascending=False)

average_scores

# Pivot again to see who missed what
pivot_df = merged_df.pivot(index="Roll Number", columns="Course", values="Weighted Score")
missing_courses = pivot_df.isnull().sum()
print("Missing courses count:")
print(missing_courses)

import plotly.express as px

fig = px.histogram(
    merged_df,
    x="Weighted Score",
    color="Course",
    barmode="overlay",
    nbins=20,
    title="Distribution of Weighted Scores by Course"
)
fig.show()

fig = px.box(
    merged_df,
    x="Course",
    y="Weighted Score",
    color="Course",
    title="Weighted Score Comparison Across Courses"
)
fig.show()

fig = px.scatter(
    merged_df,
    x="Project",
    y="Weighted Score",
    color="Course",
    hover_data=["Roll Number"],
    title="Project Marks vs Weighted Score"
)
fig.show()

student_course_counts = merged_df.groupby("Roll Number")["Course"].nunique().reset_index(name="Courses Taken")

fig = px.bar(
    student_course_counts,
    x="Roll Number",
    y="Courses Taken",
    title="Number of Courses Taken per Student"
)
fig.show()

# Pivot to show student-course matrix
pivot_df = merged_df.pivot(index="Roll Number", columns="Course", values="Weighted Score")

fig = px.imshow(
    pivot_df,
    labels=dict(x="Course", y="Roll Number", color="Weighted Score"),
    title="Weighted Score Heatmap by Student and Course"
)
fig.show()

fig = px.parallel_coordinates(
    merged_df,
    dimensions=["Quiz Total", "PA Total", "Project", "Mid Exam", "Final Exam", "Weighted Score"],
    color="Weighted Score",
    color_continuous_scale=px.colors.sequential.Viridis,
    title="Parallel Coordinates Plot of Assessment Components"
)
fig.show()

!pip install ydata-profiling

from ydata_profiling import ProfileReport

# Create the profile report
profile = ProfileReport(merged_df, title="Student Performance EDA Report", explorative=True)


# OR display inline in Jupyter Notebook
profile.to_notebook_iframe()

# Save it to an HTML file
profile.to_file("student_eda_report.html")

pivot_df = merged_df.pivot(index="Roll Number", columns="Course", values="Weighted Score")

# Create and save profile
profile_pivot = ProfileReport(pivot_df, title="Per-Student Course Comparison Report", explorative=True)
profile_pivot.to_file("student_course_comparison_report.html")

merged_df.groupby("Roll Number")["Course"].count().sort_values(ascending=False)

student_avg = merged_df.groupby("Roll Number")["Weighted Score"].mean().sort_values(ascending=False)

student_avg

sem_pivot = merged_df.pivot_table(index="Roll Number", columns="Semester", values="Weighted Score", aggfunc="mean")
sem_pivot["Change"] = sem_pivot["Sem 2"] - sem_pivot["Sem 1"]
print(sem_pivot.sort_values("Change", ascending=False))

course_summary = merged_df.groupby(["Semester", "Course"])["Weighted Score"].describe()
course_summary

avg_scores_by_course = merged_df.groupby("Course")["Weighted Score"].mean().sort_values(ascending=False)
avg_scores_by_course

avg_Weighted_Score_per_Semester = merged_df.groupby("Semester")["Weighted Score"].mean()
avg_Weighted_Score_per_Semester

import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(x="Semester", y="Weighted Score", data=merged_df)
plt.title("Score Distribution by Semester")
plt.show()

high_scorers = merged_df[merged_df["Weighted Score"] >= 80]["Roll Number"].value_counts()
high_scorers

top_per_course = merged_df.groupby("Course").apply(lambda x: x.loc[x["Weighted Score"].idxmax()])
top_per_course

pivot_course_matrix = merged_df.pivot(index="Roll Number", columns="Course", values="Weighted Score")
missing_participation = pivot_course_matrix.isnull().sum()
missing_participation

semester_count = merged_df.groupby("Roll Number")["Semester"].nunique()
one_semester_students = semester_count[semester_count == 1]
one_semester_students

correlation = merged_df[["Quiz Total", "PA Total", "Project", "Mid Exam", "Final Exam", "Weighted Score"]].corr()
sns.heatmap(correlation, annot=True, cmap="coolwarm")
plt.title("Correlation Between Assessment Components")
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Use your updated_merged_df from previous steps
# Step 1: Select relevant columns for clustering
features = ["Quiz Total", "PA Total", "Project", "Mid Exam", "Final Exam"]

# Drop rows with any missing values in these columns
cluster_df = merged_df.dropna(subset=features)

# Step 2: Normalize the feature values
scaler = StandardScaler()
scaled_features = scaler.fit_transform(cluster_df[features])

# Step 3: Determine optimal number of clusters (Elbow method)
inertia = []
for k in range(1, 10):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(scaled_features)
    inertia.append(km.inertia_)

# Plot Elbow Curve
plt.figure(figsize=(8, 4))
plt.plot(range(1, 10), inertia, marker='o')
plt.title('Elbow Method - Optimal Number of Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

# Apply clustering with chosen K
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
cluster_df["Cluster"] = kmeans.fit_predict(scaled_features)

# View few samples
print(cluster_df[["Roll Number", "Course", "Semester", "Cluster", "Weighted Score"]])

# Use PCA to reduce to 2D for visualization
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
reduced = pca.fit_transform(scaled_features)
cluster_df["PCA1"] = reduced[:, 0]
cluster_df["PCA2"] = reduced[:, 1]

# Plot clusters
plt.figure(figsize=(8,6))
sns.scatterplot(x="PCA1", y="PCA2", hue="Cluster", data=cluster_df, palette="Set2", s=100)
plt.title("Student Clusters Based on Performance Profiles")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

# Cluster-wise average profile
cluster_profiles = cluster_df.groupby("Cluster")[features + ["Weighted Score"]].mean().round(2)
cluster_profiles